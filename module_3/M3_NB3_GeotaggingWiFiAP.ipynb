{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"right\">Python 3.6 Jupyter Notebook</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Geotagging WiFi access points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "**This notebook contains advanced exercises that are only applicable to students who wish to deepen their understanding and qualify for bonus marks on this course.** \n",
    "    \n",
    "You will be able to achieve 100% for this notebook by successfully completing exercise 1. An optional, additional exercise can be completed to qualify for bonus marks.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your completion of the notebook exercises will be graded based on your ability to do the following: \n",
    "\n",
    "**All students**:\n",
    "\n",
    "> **Evaluate**: Are you able to interpret the results and justify your interpretation based on the observed data?\n",
    "\n",
    "\n",
    "**Advanced students**:\n",
    "\n",
    "> **Apply**: Are you able to execute code (using the supplied examples) that performs the required functionality on supplied or generated data sets? \n",
    "\n",
    "> **Analyze**: Are you able to pick the relevant method or library to resolve specific stated questions?\n",
    "\n",
    "> **Create**: Are you able to produce notebooks that serve as computational record of a session, and can be used to share your insights with others? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notebook objectives\n",
    "By the end of this notebook you will be expected to understand and apply the steps involved in geotagging, which are the following:\n",
    "> 1. Match the records in time.\n",
    "2. Compute the median location of each access point (AP).\n",
    "3. Filter out the non-stationary routers.\n",
    " \n",
    "####  List of exercises\n",
    ">   - **Exercise 1**: Identification of stationary WiFi routers.\n",
    "  - **Exercise 2 [Advanced]**: Identification of non-stationary WiFi routers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook introduction\n",
    "\n",
    "This notebook will use the same Dartmouth StudentLife data set, as in previous exercises. In this exercise, you will combine WiFi scans with location information to create a small database of WiFi access point (AP) locations, using Google's location services. You will be replicating the work of Piotr Sapieżyński et al. (2015).\n",
    "\n",
    "You will start by importing the necessary modules and variable definitions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<b>Note</b>:<br>\n",
    "It is strongly recommended that you save and checkpoint after applying significant changes or completing exercises. This allows you to return the notebook to a previous state should you wish to do so. On the Jupyter menu, select \"File\", then \"Save and Checkpoint\" from the dropdown menu that appears.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load libraries and set options\n",
    "\n",
    "In order to compute the median and calculate the distances in Section 1.4.2, you will need to import the custom function from the \"utils” directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load relevant libraries.\n",
    "from os import path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import folium\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "%matplotlib inline\n",
    "\n",
    "# Load custom modules.\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from utils import getmedian, haversine\n",
    "from utils import llaToECEF as coords_to_geomedian\n",
    "from utils import ECEFTolla as geomedian_to_coords\n",
    "\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define variable definitions.\n",
    "wifi_path     = '../data/dartmouth/wifi'\n",
    "location_path = '../data/dartmouth/location/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Single user review\n",
    "For simplicity, review a single user's data, examine the properties of that data, and try to see if analysis yields any results of value. You should be familiar with both WiFi and location data, from previous exercises. As a result, they will be loaded and presented without extensive clarification.\n",
    "\n",
    "#### 1.1 Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load WiFi data.\n",
    "u00_wifi = pd.read_csv(path.join(wifi_path, 'wifi_u00.csv')) \n",
    "u00_wifi.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load location data.\n",
    "u00_loc = pd.read_csv(path.join(location_path, 'gps_u00.csv'))\n",
    "u00_loc.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Remove the columns that you do not require"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove columns from WiFi dataset.\n",
    "u00_wifi.drop(['freq', 'level'], axis=1, inplace=True)\n",
    "u00_wifi.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove irrelevant columns from location dataset.\n",
    "u00_loc.drop(['provider', 'network_type', 'bearing', 'speed', 'travelstate'], axis=1, inplace=True)\n",
    "u00_loc.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Remove location records with poor accuracy\n",
    "The accuracy reported in location records is interpreted counterintuitively. The higher the value, the less accurate the measurement. It denotes the radius of a circle within which 68% of the measurements (or one standard deviation) of the reported coordinates are present. Since the radius of an outdoor access point can reach 250 metres  (Sapiezynski et al. 2015), it is safe to assume a more conservative measure of 200 metres (at an elevated risk of classifying routers as non-stationary).\n",
    "\n",
    "The accuracy of location measurements is a major source of noise, hence the need for additional consideration. To do that,  you need to plot the cumulative distribution of the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histogram of accuracy observations.\n",
    "u00_loc.accuracy.hist(cumulative=True, density=1, histtype='step', bins=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the data set contains quite accurate location measurements, as a visual inspection of the histogram suggests that almost 90% of the observations have relatively good accuracy. It is therefore safe to select only the most accurate observations.\n",
    "\n",
    "Using the Pandas \"describe\" function, you can get a quick view of the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review the dataset with Pandas decribe function.\n",
    "u00_loc.accuracy.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, determine how many observations to keep. The impact of using an accuracy value of 40 is demonstrated in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the number of records meeting our threshold of 40 for accuracy.\n",
    "result = len(u00_loc[u00_loc.accuracy <= 40]) / float(len(u00_loc))\n",
    "print('Proportion of records that meet the criteria is {:.1f}%'.format(100*result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "73% of the records meet your criteria, and will be used as a filter in subsequent steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a copy of the original dataset before applying the filter.\n",
    "u00_loc_raw = u00_loc.copy()\n",
    "\n",
    "# Apply the filter.\n",
    "u00_loc = u00_loc[u00_loc['accuracy'] <= 40]\n",
    "\n",
    "# Get the lenghts of each of the data objects.\n",
    "original_location_count = len(u00_loc_raw)\n",
    "filtered_location_count = len(u00_loc)\n",
    "\n",
    "print(\"Number of location observations before filtering: {}\".format(original_location_count)) \n",
    "print(\"Number of observations remaining after filtering: {}\".format(filtered_location_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop the accuracy column from the DataFrame, as it is no longer required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the object to remove accuracy.\n",
    "u00_loc.drop('accuracy', axis=1, inplace=True)\n",
    "\n",
    "# Display the head of the new dataset.\n",
    "u00_loc.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note**:\n",
    "\n",
    "> For certain methods, Pandas has the option of applying changes to data sets \"inplace\". While convenient, this feature should be used with care as you will no longer be able to re-execute earlier cells. The guiding principle is that you can use this feature in data cleaning and wrangling steps, where you no longer need to go back and revisit earlier steps.\n",
    "\n",
    "> Should you need to revisit earlier steps, you can either restart the notebook and execute all the cells up to that point, or only execute the cells needed to get the object in the required form to continue your analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 Geotagging\n",
    "\n",
    "In order to geotag, location and WiFi readouts need to be matched based on the time of the observations. As in the paper by Sapiezynski et al. (2015),  readouts will be constrained to those happening at exactly the same second, to reduce impact of readouts from moving vehicles.\n",
    "\n",
    "There are three steps involved in geotagging:\n",
    "1. Match the records in time.\n",
    "2. Compute the median location of each AP.\n",
    "3. Filter out the non-stationary routers.\n",
    "\n",
    "These three steps will be explored in further detail in the following sections of this notebook.\n",
    "\n",
    "#### 1.4.1 Match the records \n",
    "This requires the use of Pandas magic to join (much like SQL's join) the DataFrames based on time. First, use the time as the index with the \"`df.set_index()`\" method, and then join them with the \"`df.join()`\" method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the index for WiFi.\n",
    "u00_wifi = u00_wifi.set_index('time')\n",
    "u00_wifi.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the index for location.\n",
    "u00_loc = u00_loc.set_index('time')\n",
    "u00_loc.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having two DataFrames with time as an index, you can simply \"join\" them on the index columns by assigning the value “None” to the argument “on” as demonstrated below.\n",
    "> A [JOIN clause](http://pandas.pydata.org/pandas-docs/stable/merging.html) is used to merge DataFrames by combining rows from two or more tables, based on a common field between them. The most common type  of join is an \"inner join\". An \"inner join\" between two tables (A and B) returns all rows from A and B, where the join condition is met. That is, the intersection of the two tables.\n",
    "\n",
    "<img src=\"innerjoin.png\" alt=\"Drawing\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the two data sets, print the number of records found and display the head of the new dataset.\n",
    "u00_raw_geotags = u00_wifi.join(u00_loc, how='inner',on=None)\n",
    "print('{} WiFi records found time matching location records.'.format(len(u00_raw_geotags)))\n",
    "u00_raw_geotags.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is time to account for possible noise, and remove the routers with sparse data (i.e., less than five observations, as in the referenced paper). Pandas \"df.groupby()\" will be used to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create object u00_groups.\n",
    "u00_groups = u00_raw_geotags.groupby('BSSID')\n",
    "\n",
    "# Create a new object where filter criteria is met.\n",
    "u00_geotags = u00_groups.filter(lambda gr: len(gr)>=5)\n",
    "\n",
    "print(\"{} geotagged records remained after trimming for sparse data.\".format(len(u00_geotags)))\n",
    "print(\"They correspond to {} unique router APs\".format(len(u00_groups)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4.2 Compute the median location of each AP\n",
    "**Define stationary routers** as ones for which 95% of observations fall inside a radius of 200 metres from the geometric median of all of the observations. In order to compute the median and calculate the distances, you will need to import the custom function from the \"utils” directory.\n",
    "\n",
    "In order to compute the geometric medians with the tools at your disposal, the \"`getmedian()`\" method needs properly-formatted data. That means a lot of list points, where each point is an array of \"longitude\", \"latitude\", and \"altitude\". The algorithm accepts input in degrees as units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a new DataFrame with latitude and longitude.\n",
    "u00_geo_medians = pd.DataFrame(columns=[u'latitude', u'longitude'])\n",
    "\n",
    "# Transform the data set using the provided set of utilities.\n",
    "for (BSSID, geotags) in u00_groups:\n",
    "    \n",
    "    geotags = [row for row in np.array(geotags[['latitude', 'longitude', 'altitude']])]\n",
    "    geotags = [coords_to_geomedian(row) for row in geotags]\n",
    "    \n",
    "    median  = getmedian(geotags)\n",
    "    median  = geomedian_to_coords(median)[:2]\n",
    "    \n",
    "    u00_geo_medians.loc[BSSID] = median"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After completing the above, you will have your geomedians, and will be ready to move on to the last step, which is to filter out the non-stationary access points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the head of the geomedians object.\n",
    "u00_geo_medians.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4.3 Filter out the non-stationary routers\n",
    "**Identify stationary routers** with 95% confidence, and a distance threshold of 200 metres. Start by computing the distances using the \"`haversine()`\" function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate the distances from the median.\n",
    "u00_distances = {}\n",
    "for BSSID, geotags in u00_groups:\n",
    "    u00_distances[BSSID] = []\n",
    "    (lat_median, lon_median) = u00_geo_medians.loc[BSSID]\n",
    "    for (lat, lon) in np.array(geotags[['latitude','longitude']]):  \n",
    "        u00_distances[BSSID].append(haversine(lon, lat, lon_median, lat_median)*1000) # haversine() returns distance in [km]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, check how many of the routers pass the threshold. Iterate over the access points, and count the ratio of measurements outside the threshold to all measurements. They are assigned to \"static\" or \"others\" based on your confidence level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group access points as static or non-static.\n",
    "# Set the thresholds.\n",
    "distance_threshold = 200\n",
    "confidence_level    = 0.95\n",
    "\n",
    "# Create empty lists.\n",
    "static = []\n",
    "others = []\n",
    "\n",
    "for BSSID, distances in u00_distances.items():\n",
    "    \n",
    "    all_count  = len(distances)\n",
    "    near_count = len(list(filter(lambda distance: distance <= distance_threshold, distances)))\n",
    "    \n",
    "    if( near_count / all_count >= confidence_level ):\n",
    "        static.append(BSSID)\n",
    "    else:\n",
    "        others.append(BSSID)\n",
    "\n",
    "# Print summary results.\n",
    "print(\"We identified {} static routers and {} non-static (moved or mobile).\".format(len(static), len(others))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tagged routers (access points) can now be visualized on a map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the access points on a map.\n",
    "map_center  = list(u00_geo_medians.median())\n",
    "routers_map = folium.Map(location=map_center, zoom_start=14)\n",
    "# Add points to the map for each of the locations.\n",
    "for router in static:\n",
    "      folium.CircleMarker(u00_geo_medians.loc[router], fill_color='red', radius=15, fill_opacity=0.5).add_to(routers_map)\n",
    "\n",
    "#Display the map.\n",
    "routers_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note**:\n",
    "\n",
    "> In order to validate your results, you can compare the location with the known location (lat: 43.7068263, long:-72.2868704)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set the provided location.\n",
    "lat = 43.7068263\n",
    "lon = -72.2868704\n",
    "\n",
    "bssid1 = '00:01:36:57:be:88'\n",
    "bssid2 = '00:01:36:57:be:87'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now compare this to your computed values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u00_geo_medians.loc[[bssid1, bssid2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are acceptable. You can compute the actual distance between the points with the \"haversine\" function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and display the difference between calculated and Google API provided locations.\n",
    "lat_m1, lon_m1 = u00_geo_medians.loc[bssid1]\n",
    "lat_m2, lon_m2 = u00_geo_medians.loc[bssid2]\n",
    "\n",
    "print('Distance from the Google API provided location to our first router ' \\\n",
    "            'estimation is {:2g}m'.format(haversine(lon,lat,lon_m1,lat_m1)*1000))\n",
    "print('Distance from the Google API provided location to our first router ' \\\n",
    "            'estimation is {:2g}m'.format(haversine(lon,lat,lon_m2,lat_m2)*1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Review of all users\n",
    "Next, repeat the analysis from the previous section for all users. This analysis will be used in the next exercise.\n",
    "\n",
    "<br>\n",
    "<div class=\"alert alert-warning\">\n",
    "<b>Important:</b>\n",
    "Please ensure that this is the only running notebook when performing this section, because you will require as many resources as possible to complete the next section. In the Orientation Module, you were introduced to the process required to shut down notebooks. That being said, you can shut down running notebooks by viewing the \"Running\" tab on your Jupyter Notebook directory view.\n",
    "</div>\n",
    "\n",
    "> **Note**:\n",
    "\n",
    "> There will be less contextual information provided in this section, as the details have already been provided in Section 1 of this notebook\n",
    "\n",
    "#### 2.1 Load data for all users\n",
    "This section utilizes two new libraries that do not fall within the scope of this course. Interested students can read more about [glob](https://docs.python.org/2.7/library/glob.html),  which is used to read files in the specified directory, and [tqdm](https://github.com/tqdm/tqdm), which is  used to render a progress bar. Set your variables, then create the required function to process the input files, and finally, execute this function to process all the files.\n",
    "\n",
    "> **Note**:\n",
    "\n",
    "> Processing a large amount of files or records can be time consuming. It is good practice to include progress bars to provide visual feedback where applicable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set variables.\n",
    "all_geotags = pd.DataFrame(columns=['time','BSSID','latitude','longitude','altitude'])\n",
    "all_geotags = all_geotags.set_index('time')\n",
    "pcounter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define function to build the dataset, all_geotags, using the input files supplied.\n",
    "def build_ds(file_in, all_geotags):\n",
    "\n",
    "    # Get the user id.\n",
    "    user_id = path.basename(file_in)[5:-4]\n",
    "\n",
    "    # Read the WiFi and location data for the user.\n",
    "    wifi = pd.read_csv(file_in)\n",
    "    loc  = pd.read_csv(path.join(location_path, 'gps_'+user_id+'.csv'))\n",
    "\n",
    "    # Filter location data not meeting the accuracy threshold.\n",
    "    loc = loc[loc.accuracy <= 40]\n",
    "\n",
    "    # Drop the columns not required.\n",
    "    wifi.drop(['freq', 'level'], axis=1, inplace=True)\n",
    "    loc.drop(['accuracy', 'provider', 'network_type', 'bearing', 'speed', 'travelstate'], axis=1, inplace=True)\n",
    "\n",
    "    # Index the datasets based on time.\n",
    "    loc  = loc.set_index('time')\n",
    "    wifi = wifi.set_index('time')\n",
    "\n",
    "    # Join the datasets based on time index.\n",
    "    raw_tags = wifi.join(loc, how='inner')\n",
    "\n",
    "    # Return the dataset for the user.\n",
    "    return [raw_tags]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through the files in the specified directory and append the results of the function to the all_geotags variable.\n",
    "for f in tqdm(glob.glob(wifi_path + '/*.csv')):\n",
    "    # Append result from our function to all_geotags for each input file supplied.\n",
    "    all_geotags = all_geotags.append(build_ds(f, all_geotags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Drop APs with sparse records\n",
    "Remove access points with less than five observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"{} all geotags found\".format(len(all_geotags)))\n",
    "\n",
    "all_groups = all_geotags.groupby('BSSID')\n",
    "print(\"{} unique routers found\".format(len(all_groups)))\n",
    "\n",
    "# Drop sparsely populated access points.\n",
    "all_geotags = all_groups.filter(lambda gr: len(gr)>=5)\n",
    "all_groups = all_geotags.groupby('BSSID')\n",
    "\n",
    "print(\"{} unique router APs remaining after dropping routers with sparse data\".format(len(all_groups)))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Compute medians\n",
    "Compute the medians for each router in the combined data set, as per Section 1.4.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new variable containing all the coordinates.\n",
    "all_geo_medians = pd.DataFrame(columns=[u'latitude', u'longitude'])\n",
    "\n",
    "# Compute the geomedians and add to all_geo_medians.\n",
    "\n",
    "# Initiate progress bar.\n",
    "with tqdm(total=len(all_groups)) as pbar:\n",
    "    # Iterate through data in all_groups as per single user example.\n",
    "    for i, data in enumerate(all_groups):\n",
    "        (BSSID, geotags) = data\n",
    "\n",
    "        geotags = [row for row in np.array(geotags[['latitude', 'longitude', 'altitude']])]\n",
    "        geotags = [coords_to_geomedian(row) for row in geotags]\n",
    "\n",
    "        median  = getmedian(geotags)\n",
    "        median  = geomedian_to_coords(median)[:2]\n",
    "\n",
    "        all_geo_medians.loc[BSSID] = median\n",
    "        pbar.update()\n",
    "    pbar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 Compute distances of observations to the calculated median\n",
    "Compute the distance from the medians for each router in the combined dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the distances from the median.\n",
    "all_distances = {}\n",
    "\n",
    "# Initiate progress bar.\n",
    "with tqdm(total=len(all_groups)) as pbar:\n",
    "    # Iterate through data in all_groups as per single user example.\n",
    "    for i, data in enumerate(all_groups):\n",
    "        (BSSID, geotags) = data\n",
    "        all_distances[BSSID] = []\n",
    "        (lat_median, lon_median) = all_geo_medians.loc[BSSID]\n",
    "        for (lat, lon) in np.array(geotags[['latitude','longitude']]):  \n",
    "            all_distances[BSSID].append(haversine(lon, lat, lon_median, lat_median)*1000)\n",
    "        pbar.update()\n",
    "    pbar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5 Label APs as static or non-static"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group access points as static or non-static.\n",
    "# Set the thresholds.\n",
    "distance_threshold = 200\n",
    "confidence_level   = 0.95\n",
    "\n",
    "# Create empty lists.\n",
    "all_static = []\n",
    "all_others = []\n",
    "\n",
    "for BSSID, distances in all_distances.items():\n",
    "    \n",
    "    all_count  = len(distances)\n",
    "    near_count = len(list(filter(lambda distance: distance <= distance_threshold, distances)))\n",
    "    \n",
    "    if( near_count / all_count >= confidence_level ):\n",
    "        all_static.append(BSSID)\n",
    "    else:\n",
    "        all_others.append(BSSID)\n",
    "\n",
    "# Print summary results.\n",
    "print(\"We identified {} static routers and {} non-static (moved or mobile).\".format(len(all_static), len(all_others))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.6 Plot the static APs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot the access points on a map.\n",
    "all_map_center  = list(all_geo_medians.median())\n",
    "all_routers_map = folium.Map(location=all_map_center, zoom_start=10)\n",
    "\n",
    "# Add 1000 randomly sampled points to a new variable.\n",
    "random.seed(3)\n",
    "rn_static = random.sample(all_static,1000)\n",
    "\n",
    "# Add the points in rn_static to the map. A random seed value is used for reproducibility of results.  \n",
    "for router in rn_static:\n",
    "    folium.CircleMarker(all_geo_medians.loc[router], fill_color='red', radius=15, fill_opacity=0.5).add_to(all_routers_map)\n",
    "\n",
    "# Display the map.\n",
    "all_routers_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<div class=\"alert alert-info\">\n",
    "<b>Exercise 1 Start.</b>\n",
    "</div>\n",
    "\n",
    "### Instructions\n",
    "\n",
    "> Review the visual output produced for all users. Try to find a static router located outside North America. Remember that you can scroll and zoom on the map."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Question**: Are you able to locate a static router located outside North America? If so, where? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your markdown answer here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "<b>Exercise 1 End.</b>\n",
    "</div>\n",
    "\n",
    "> **Exercise complete**:\n",
    "    \n",
    "> This is a good time to \"Save and Checkpoint\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<div class=\"alert alert-info\">\n",
    "<b>Exercise 2 [Advanced] Start.</b>\n",
    "</div>\n",
    "\n",
    "> <div class=\"alert alert-warning\">\n",
    "<b>Note</b>:<br>\n",
    "This activity is for advanced students only and extra credit will be allocated. Students will not be penalized for not completing this activity.\n",
    "</div>\n",
    "\n",
    "### Instructions\n",
    "\n",
    "> Can you identify moving BSSIDs and plot the observed locations on a map?\n",
    "\n",
    "> This is not a trivial task (compared to geotagging stationary routers) and there are many possible ways to perform the analysis.\n",
    "\n",
    "> Input : All data points for an access point. \n",
    "\n",
    "> Output: Boolean mobility status.  \n",
    "    \n",
    "> 1. Perform agglomerative clustering. (This is not covered in the scope of this course.)\n",
    "> 2. Discard clusters with n<5 observations as noise.\n",
    "> 3. Compare pairwise timeframes in which each of the clusters were observed. \n",
    "> 4. If at least two clusters overlap (in time), consider the AP as mobile.\n",
    "    \n",
    "> **Note**:\n",
    "\n",
    "> Keep in mind that other, possibly better, solutions exist, and you are encouraged to provide your input.\n",
    "\n",
    "> **Hints**:\n",
    "\n",
    "> You can start by reviewing [SciPy](https://www.scipy.org/).\n",
    "\n",
    "> - `from scipy.spatial.distance import pdist`\n",
    "\n",
    "> - `from scipy.cluster.hierarchy import linkage, fcluster`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your answer here.\n",
    "#  Please add as many cells as you require in this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your plot here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "<b>Exercise 2 [Advanced] End.</b>\n",
    "</div>\n",
    "\n",
    "> **Exercise complete**:\n",
    "    \n",
    "> This is a good time to \"Save and Checkpoint\".\n",
    "\n",
    "> It is strongly recommended that you select \"Close and Halt\" when you have completed this notebook, to ensure that it does not continue to consume available resources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Submit your notebook\n",
    "\n",
    "Please make sure that you:\n",
    "- Perform a final \"Save and Checkpoint\";\n",
    "- Download a copy of the notebook in \".ipynb\" format to your local machine using \"File\", \"Download as\", and \"IPython Notebook (.ipynb)\"; and\n",
    "- Submit a copy of this file to the Online Campus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. References\n",
    "Sapieżyński, Piotr, Radu Gatej, Alan Mislove, and Sune Lehmann. 2015. “Opportunities and Challenges in Crowdsourced Wardriving.” Proceedings of the 2015 ACM Conference on Internet Measurement Conference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
